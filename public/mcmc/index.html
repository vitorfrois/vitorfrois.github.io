<!doctype html>
<html lang="en">
  <script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><div id="nav-border" class="container">
  <nav id="nav" class="nav justify-content-center">
    <table style="table-layout: fixed; margin-bottom: -2px;">
      <tbody>
        <tr>
          
          
          
          <td><a class="nav-link" href="/"><i data-feather="home"></i> Home</a></td>
          
          
          
          <td><a class="nav-link" href="/blog/"><i data-feather="edit"></i> Blog</a></td>
          
          
          
          <td><a class="nav-link" href="/tags/"><i data-feather="tag"></i> Tags</a></td>
          
          
          
          <td><a class="nav-link" href="/resume/resume.pdf"><i data-feather="resume"></i> Resume</a></td>
          
        </tr>
      </tbody>
    </table>
  </nav>
</div>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="generator" content="Hugo 0.139.3">
  
  
  <link rel="stylesheet" href="/css/monospace.css">
  
  
  
  <table class="header">
    <tr>
      <th colspan="4">
        <h1 class="title"> Vítor Fróis </h1>
        <span class="subtitle"> Personal Blog </span>
      </th>
    </tr>
    <tbody>
      <tr>
        <th>Title</th>
        <td colspan="3"> Monte Carlo Markov Chains and the Metropolis Algorithm </td>
      </tr>
      <tr>
        <th>Date</th>
        <td class="width-min" colspan="3">

<i data-feather="calendar"></i> <time datetime="2024-07-18">Jul 18, 2024</time>

</td>
      </tr>
    </tbody>
  </table>
  
</head>
  <body>
    <div class="container">
      <main id="main">
       

<script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [["$$","$$"],["\\(","\\)"]]} })
</script>
</script>

<main>
    
    <h2 id="montes-claros-monte-carlo-method"><del>Montes Claros</del> Monte
Carlo method</h2>
<p>When talking about stochastic experiments, it is important to know
different realizations can lead to different final results. Monte Carlo
method proposes to repeat the experiment and measure the average between
them.</p>
<p>If we throw a dice, the expected value is 3.5. Altough, in a single
realization, this could not be the result. After throwing a dice a
million times and averaging the obtained values, we should come to a
result equal to the theoretical one.</p>
<h2 id="markov-chains">Markov Chains</h2>
<p>A Markov Chain is defined by a serie of events in which the
probability of the next event will depend only on the probability of the
state of the current event. This will be a expressive difference from
the Monte Carlo method where the samples are statistically independent,
those in MCMC are correlated: as you might have expected, the next
sample depends on the current one.</p>
<h2 id="monte-carlo-markov-chains">Monte Carlo Markov Chains</h2>
<p>Or MCMC, and they are not exactly the same thing as traditional Monte
Carlo method. The are a class of algorithms for <strong>sampling
probability distributions</strong>. Each Markov step in the process is
sampling the target distribution, and then, the more steps, more
samples, and more accurate is the method.</p>
<p>In other words, given a unknown distribution, we can construct a set
of samples that lies on the range of possible outcomes and spend a
amount of (markovian) steps in each state/interval proportional to the
actual density of the distribution.</p>
<h3 id="very-simple-mcmc-example">Very simple MCMC example</h3>
<p>Lets say we want to discover the probability of sunny P(S) and rainy
P(R) days at Montes Claros. We have some information, conditional
probabilities about the weather.</p>
<p><span class="math display">\[P(S_{t+1}|R_t) = 0.5\]</span> <span
class="math display">\[P(R_{t+1}|R_t) = 0.5\]</span> <span
class="math display">\[P(R_{t+1}|S_t) = 0.1\]</span> <span
class="math display">\[P(S_{t+1}|S_t) = 0.9\]</span></p>
<p>The system is irreducible (there is a single communicating state - or
I can reach any state out of anywhere) and aperiodic (there is non
defined pattern for visiting states). Here, we begin at a random state
and randomly walk through the Markov Chain. At an infinitely high number
of states we obtain a stationary distribution <span
class="math inline">\(\pi\)</span> with <span
class="math inline">\(\pi(S)=0.833\)</span> and <span
class="math inline">\(\pi(S)=0.166\)</span></p>
<h2 id="metropolis-algorithm">Metropolis Algorithm</h2>
<p>MCMC are only a class of methods that have diverse implementations.
One of the most common implemetations is the Metropolis Algorithm.</p>
<p>Lets remember our objective is to sample from a unknown distribution,
lets call it <span class="math inline">\(p(x)\)</span>. We want to
estimate it given a similar distribution <span
class="math inline">\(f(x)\)</span> where they differ only by a
constant, this is <span class="math inline">\(p(x)=f(x)/C\)</span>. MCMC
suggests that after some <strong>burn-in</strong> steps (these are like
steps until we get in track), we will be sampling from <span
class="math inline">\(p(x)\)</span>.</p>
<p><span class="math display">\[
\begin{equation}
X_0 \rightarrow X_1 \rightarrow ... \rightarrow X_n \rightarrow X_{n+1}
\\\
\end{equation}
\]</span></p>
<p>Looks easy. Below, we will see that given a state <span
class="math inline">\(a\)</span> and a candidate <span
class="math inline">\(b\)</span>, we <strong>should</strong> go from
<span class="math inline">\(a\)</span> to <span
class="math inline">\(b\)</span> if <span
class="math inline">\(b\)</span> is more probable. And if the state
<span class="math inline">\(b\)</span> is less probable, we
<strong>maybe</strong> can go, according to the acceptance
probability.</p>
<h3 id="sample-from-an-easier-distribution-g">1. Sample from an easier
distribution <span class="math inline">\(g\)</span></h3>
<p>Normal looks fine. Lets compute <span
class="math inline">\(g(x_{t+1}|x_t) = N(x_t, \sigma ^2 )\)</span>.<span
class="math inline">\(g(x_{t+1}|x_t)\)</span> is not our definite next
step. Actually, we need to compute an acceptance probability.</p>
<h3 id="accept-or-not-the-sample">2. Accept (or not) the sample</h3>
<p>Call the acceptance probability <span class="math inline">\(A(x_t
\rightarrow x_{t+1})\)</span> and the transition probability <span
class="math inline">\(T(x_t \rightarrow x_{t+1})\)</span></p>
<p>If the so called detailed balanced equation <span
class="math display">\[p(a)T(a \rightarrow b) = p(b)T(b \rightarrow
a)\]</span> is true, then it should be garanteed that MCMC converges to
our <span class="math inline">\(p(x)\)</span>.</p>
<p>We have to substitute <span class="math inline">\(p(x)\)</span> in
the equation and the transition probability by the probability of
proposing the state change and probability of accepting. It follows:
<span class="math display">\[
\frac{f(a)}{C} g(b|a)A(a \rightarrow b) = \frac{f(b)}{C} g(a|b)A(b
\rightarrow a)
\]</span></p>
<p>Rewriting using ratios (the constant C will cancel): <span
class="math display">\[
\frac{A(a \rightarrow b)}{A(b \rightarrow a)} = \frac{f(b)}{f(a)}
\frac{g(a|b)}{g(b|a)}
\]</span></p>
<p>And finally naming the right side fractions: <span
class="math display">\[
\frac{A(a \rightarrow b)}{A(b \rightarrow a)} = r_f r_g
\]</span></p>
<p>Finally, we can execute the acceptance or not of our state
transition. - if <span class="math inline">\(r_f r_g \lt 1\)</span> then
<span class="math display">\[
    A(a \rightarrow b) = r_f r_g \\\
    A(b \rightarrow a) = 1
\]</span> - else <span class="math display">\[
    A(a \rightarrow b) = 1 \\\
    A(b \rightarrow a) = \frac{1}{r_f r_g}
\]</span></p>
<h3 id="metropolis-case">3. Metropolis case</h3>
<p>In the metropolis case (differently from the Metropolis-Hastings
algorithm), <span class="math inline">\(g\)</span> is a simmetric
probability distribution - a Gaussian e.g. Then, <span
class="math inline">\(g(a|b) = g(b|a)\)</span>, allowing us to rewrite
the acceptance function as</p>
<p><span class="math display">\[
A(a \rightarrow b) = max(1, \frac{f(b)}{f(a)}) = max(1,
\frac{p(b)}{p(a)})
\]</span></p>
<h2 id="applications-and-conclusions">Applications and conclusions</h2>
<p>MCMC is widely used for approximating numerical solutions for
integrals and draw posterior samples in bayesian inference. Metropolis
algorithm is widely used in simulations, and I pretend to follow this
post with an example of its usage and implementation in the Ising model
simulation.</p>
<h2 id="references">References</h2>
<ul>
<li><a
href="https://blog.djnavarro.net/posts/2023-04-12_metropolis-hastings/">Data
Witch Metropolis Hastings</a></li>
<li><a href="https://www.youtube.com/watch?v=yCv2N7wGDCw">ritvikmath
Metropolis-Hastings</a></li>
<li><a href="https://www.youtube.com/watch?v=7QX-yVboLhk">Michael Pyrcz
MCMC</a></li>
</ul>

</main>

      </main>
    </div>
    <script src="/static/js/index.js"></script>
  </body>
  <footer class="footer container h-10 text-center mt-1">
    <hr>
    <ul class="pl-0 mt-1" style="text-align: center;">
        <a href="https://github.com/vitorfrois/vitorfrois.github.io">Code</a>
        <span class="ml-2">©  2024</span>
    </ul>
</footer>

</html>

